CartPole-v0:
  gamma: 0.99  #奖励的衰减因子，决定了未来奖励的价值，一般在 0 和 1 之间。
  batch_size: 128  # 每次训练迭代中，从经验回放库中随机抽取的样本数量
  hidden_size: 128  # 神经网络隐藏层的神经元数量。
  hidden_size2: 128

  replay_size: 1.0e+4  #经验回放库的大小，即可以存储的最大经验数。
  replay_size_start: 10000  #在开始训练前，回放库中所需的最小经验数。

  epsilon_decay: 5000  # ε-greedy 策略中 ε 的衰减速率。
  epsilon_final: 0.01 # ε-greedy 策略中 ε 的最终值。
  epsilon_start: 1.0 # ε-greedy 策略中 ε 的初始值,表示一开始有多少概率进行随机行动

  learning_rate: 1.0e-3 # 神经网络的学习率，用于更新网络权重。


  stopping_reward: 195 # 当平均奖励达到这个阈值时，训练停止。
  max_frames: 2.0e+5  # 训练过程中的最大帧数。

  save_path: saved_models  # 保存模型的路径。

  target_net_sync: 1.0e+3 # 目标网络的同步频率，即每隔多少步更新一次目标网络。
  tau: 0.005 # 目标网络的软更新系数，用于更新目标网络的权重。
  alpha_sync: false  # 是否使用软更新。

  clip_gradient: false  # 是否在优化过程中对梯度进行裁剪以防止梯度爆炸。

CartPole-v1:
  gamma: 0.99  #奖励的衰减因子，决定了未来奖励的价值，一般在 0 和 1 之间。
  batch_size: 128  # 每次训练迭代中，从经验回放库中随机抽取的样本数量
  hidden_size: 128    # 神经网络隐藏层的神经元数量。
  hidden_size2: 128   # 神经网络隐藏层的神经元数量。

  replay_size: 1.0e+6  #经验回放库的大小，即可以存储的最大经验数。
  replay_size_start: 5000   #在开始训练前，回放库中所需的最小经验数。

  epsilon_decay: 50000  # ε-greedy 策略中 ε 的衰减速率。
  epsilon_final: 0.001  # ε-greedy 策略中 ε 的最终值。
  epsilon_start: 0.250  # ε-greedy 策略中 ε 的初始值,表示一开始有多少概率进行随机行动

  learning_rate: 5.0e-4  # 神经网络的学习率，用于更新网络权重。

  target_net_sync: 1.0e+3  # 目标网络的同步频率，即每隔多少步更新一次目标网络。

  stopping_reward: 475  # 当平均奖励达到这个阈值时，训练停止。
  max_frames: 5.0e+5  # 训练过程中的最大帧数。

  save_path: saved_models

  tau: 0.005  # 目标网络的软更新系数，用于更新目标网络的权重。
  alpha_sync: true  # 是否使用软更新。

  clip_gradient: false  # 是否在优化过程中对梯度进行裁剪以防止梯度爆炸。

LunarLander-v2:
  gamma: 0.99  #奖励的衰减因子，决定了未来奖励的价值，一般在 0 和 1 之间。
  batch_size: 128  # 每次训练迭代中，从经验回放库中随机抽取的样本数量
  hidden_size: 256  # 神经网络隐藏层的神经元数量。
  hidden_size2: 300  # 神经网络隐藏层的神经元数量。

  replay_size: 1.0e+6  #经验回放库的大小，即可以存储的最大经验数。
  replay_size_start: 5000  #在开始训练前，回放库中所需的最小经验数。

  epsilon_decay: 50000  # ε-greedy 策略中 ε 的衰减速率。
  epsilon_final: 0.001  # ε-greedy 策略中 ε 的最终值。
  epsilon_start: 1.0  # ε-greedy 策略中 ε 的初始值,表示一开始有多少概率进行随机行动

  learning_rate: 1.0e-3   # 神经网络的学习率，用于更新网络权重。

  target_net_sync: 1.0e+3  # 目标网络的同步频率，即每隔多少步更新一次目标网络。
  alpha_sync: false  # 是否使用软更新。
  tau: 0.005  # 目标网络的软更新系数，用于更新目标网络的权重。

  stopping_reward: 200  # 当平均奖励达到这个阈值时，训练停止。
  max_frames: 1.0e+6  # 训练过程中的最大帧数。

  save_path: saved_models

  clip_gradient: false  # 是否在优化过程中对梯度进行裁剪以防止梯度爆炸。

